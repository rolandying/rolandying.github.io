---
---

@article{yin2022rgb,
  title={RGB-D-Based Robotic Grasping in Fusion Application Environments},
  author={Yin, Ruochen and Wu, Huapeng and Li, Ming and Cheng, Yong and Song, Yuntao and Handroos, Heikki},
  journal={Applied Sciences},
  volume={12},
  number={15},
  pages={7573},
  year={2022},
  publisher={MDPI},
  preview={grasping.gif}
}

@article{yin2023monocular,
  title={Monocular Camera-Based Robotic Pick-and-Place in Fusion Applications},
  author={Yin, Ruochen and Wu, Huapeng and Li, Ming and Cheng, Yong and Song, Yuntao and Handroos, Heikki},
  journal={Applied Sciences},
  volume={13},
  number={7},
  pages={4487},
  year={2023},
  publisher={MDPI},
  preview={pp.jpg}
}

@article{yin2020fusionlane,
  title={Fusionlane: Multi-sensor fusion for lane marking semantic segmentation using deep neural networks},
  author={Yin, Ruochen and Cheng, Yong and Wu, Huapeng and Song, Yuntao and Yu, Biao and Niu, Runxin},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={23},
  number={2},
  pages={1543--1553},
  year={2020},
  publisher={IEEE},
  preview={FL.jpg}
}

@article{yin2022mastering,
  title={Mastering Autonomous Assembly in Fusion Application with Learning-by-doing: a Peg-in-hole Study},
  author={Yin, Ruochen and Wu, Huapeng and Li, Ming and Cheng, Yong and Song, Yuntao and Handroos, Heikki},
  journal={arXiv preprint arXiv:2208.11737},
  year={2022},
  abstract={Robotic peg-in-hole assembly represents a critical area of investigation in robotic automation. The fusion of reinforcement learning (RL) and deep neural networks (DNNs) has yielded remarkable breakthroughs in this field. However, existing RL-based methods grapple with delivering optimal performance under the unique environmental and mission constraints of fusion applications. As a result, we propose an inventively designed RL-based approach. In contrast to alternative methods, our focus centers on enhancing the DNN architecture rather than the RL model. Our strategy receives and integrates data from the RGB camera and force/torque (F/T) sensor, training the agent to execute the peg-in-hole assembly task in a manner akin to human hand-eye coordination. All training and experimentation unfold within a realistic environment, and empirical outcomes demonstrate that this multi-sensor fusion approach excels in rigid peg-in-hole assembly tasks, surpassing the repeatable accuracy of the robotic arm utilized—0.1 mm—in uncertain and unstable conditions.},
  video={Vna5jrJm85I},
  preview={assembly.jpg}
}
